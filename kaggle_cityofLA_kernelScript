---
title: "A corpus-based method for detecting gender/racial semantic alignment in job announcement texts"
author: "BenjS"
date: "June 19, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", message = FALSE, warning = FALSE)
```
# Introduction

## Problem background

The City of Los Angeles has a lot of hiring to do in the coming years. To ensure that positions are filled with the best possible talent, job bulletins must be worded in a way that attracts a diverse pool of applicants. But how to assess potential bias in job bulletin wordings?

Numerous peer reviewed studies have examined job announcement texts for gender bias (Tang et al., 2017; Hannak et al. 2017; Askehave & Zethsen, 2014; Gaucher, Friesen, & Kay 2011; Bem & Bem 1973). The general finding is that many job announcements tend to be worded in a way that biases the applicant pool towards male applicants.

The main theory emerging from these studies is that bias towards male applicants occurs through "gendered" wordings that embed conceptual gender stereotypes into language (Tang et al. 2017). Lists of such gendered words and phrases have been ellicited through surveys, and then subsequently compiled and curated (Gaucher, Friesen, & Kay 2011). Generally speaking, these lists associate females with communal, inclusive, caring traits, and males with aggresive, dominant, exclusive, agentic traits. The two state of the art gender bias detection services in existence today, Textio and Unitive, are based on these lists. Textio and Unitive charge a non-trivial fee for access to their services, although Tang et al. (2017) have devised an algorithm that accurately mimicks the services.

## Problems with current methods

There are a few problems with the gendered list-based approach. One of these is that gender stereotypes evolve over time. Recent evidence suggests that female job applicants today do not necessarily identify with lists of traits ellicited two decades ago (Donnelly & Twenge 2017). In order to accurately reflect biases embedded in today's language, lists must therefore be continually updated. Ideally, such updates would be based on new, periodic surveys. But surveys are typically limited to a particular age group (university students, for eg.) in a particular geographical area, and thus offer but a glimpse of the potential pool of female applicants. Surveys are also probably too expensive and time consuming to be conducted on a periodic basis. In lieu of surveys, lists might be modified by their curators or "experts", but this introduces a regrettable degre of arbitrariness and subjective bias into the method. Given the proprietary, confidential nature of the Textio/Unitive business model, we cannot know if and/or how these companies update their lists. This lack of transparency is itself a further drawback of the list-based approach.

## Proposed solution: COCA-based semantic bias detection

In this kernel, I propose a new gender bias detection method that is transparent, list-free, self-updating, and free of charge. It can also be used to detect racial bias, or bias with respect to any two user-defined "semantic poles". Semantic poles can be defined by as few or as many words as the user would like. In the case of gender bias detection, the poles might be defined by "feminine", "communal", "inclusive" on the one hand, and "masculine", "dominant", "exclusive", on the other. (Curated lists can thus inform the definition of semantic poles, but the prposed method is not dependent on such lists.)

Instead of lists, the proposed method is based on the Corpus of Contemporary American English (COCA, 2018), which is continually updated to accurately reflect current common American English usage across a wide range of text modalities--academic, magazine, news, fiction, and spoken. The COCA is taken as the semantic baseline, and then a job bulletin's (or any given text's) semantic alignment with respect to the user-defined poles is measured against this baseline.

Numerical measurement of semantic alignment is achieved by first converting words to vectors via Google's word2vec function. Word vectors effectively encode the "semantic position" of the word with respect to every other word in the corpus, allowing words to be added and subtracted, as in the following classic example:

$$
\mathbf{w}_{queen} \approx  \mathbf{w}_{king} - \mathbf{w}_{man} + \mathbf{w}_{woman}
$$
where $\mathbf{w}_{xyz}$ represents the vector of the word "xyz".

Words can also be averaged. When the user defines a semantic pole by more than one word, the pole is defined as the average of these word vectors. For example, if $\mathbf{w}_f$ represents the feminine semantic pole mentioned above then
$$
\mathbf{w}_f = \frac{1}{3}(\mathbf{w}_{feminine}+\mathbf{w}_{communal}+\mathbf{w}_{inclusive})
$$
By contrast, the masculine semantic pole would be defined
$$
\mathbf{w}_m = \frac{1}{3}(\mathbf{w}_{masculine}+\mathbf{w}_{dominant}+\mathbf{w}_{exclusive})
$$
Let the average word of an entire job bulletin be defined
$$
\mathbf{w}_b = \frac{1}{n} \sum_i^n\mathbf{w}_i
$$
where $\mathbf{w}_i$ is the $i^{th}$ word in the bulletin.

For each LA City job bulletin, the proposed method takes the bulletin's average word and then computes its closeness (cosine similarity) to the two user-defined semantic poles. The difference between these two distances is then intrepreted as the bulletin's "semantic alignment" with respect to the two poles. For example,
$$
\mathrm{Feminine}\:\:\mathrm{Semantic}\:\:\mathrm{Alignment}=\cos(\theta(\mathbf{w}_b,\mathbf{w}_f)) - \cos(\theta(\mathbf{w}_b,\mathbf{w}_m))
$$
where
$$
\cos(\theta(\mathbf{w}_x,\mathbf{w}_y)) = \frac{\mathbf{w}_x\cdot\mathbf{w}_y}{\|\mathbf{w}_x\|\|\mathbf{w}_y\|}
$$

The negative of the feminine semantic alignment can be interpreted as the masculine semantic alignment score.

## Summary of results

The highest feminine semantic alignment scores correspond to lower paying, junior, or assistant level job classes (including clerical and 
nursing roles), while the highest masculine alignment scores correspond to higher paying, senior level or supervisory job classes. 
The mean salary of job classes with high feminine semantic alignment is considerably less than that of job classes with the high masculine semantic alignment. This disparity is more pronounced for DWP salaries than for GEN salaries.

In regression analysis, I find a roughly 1 percent (+/- 0.7%) decrease in entry salary per marginal increase in the feminine semantic alignment score $(p < 0.001)$.

Word count and sentence count bear a statistically significant relation to the alignment score. Higher word counts are associated with higher masculine alignment, while higher sentence counts are associated with higher feminine alignment, suggesting that shorter job bulletins and/or shorter sentences (i.e., the same information broken up into more sentences) may attract more female applicants.

## Kernel section organization

The kernel is organized as follows:

1 Data cleaning
2 Retrieving information from the job bulletins, generating the job class .csv output file, and extending the data dictionary
2.1 New fields added to the output file
2.1.1 New salary fields
2.1.2 Word/sentence counts, unique text fields
2.3 Missing output fields
2.4 Here we go
3 Semantic alignment analysis of LA City job bulletins
3.1 Assessment of feminine/masculine semantic alignment
3.2 Assessment of racial semantic alignment

To get started, below I load/define required libraries and folders. I also define the many functions needed to retrieve information from the job bulletins.

```{r}
options(warn = -1, message = FALSE)
#======================================================
bulletins_folder <- "../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Job Bulletins/"
dataDictionary_file_path <- "../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Additional data/kaggle_data_dictionary.csv"
COCA_folder <- "../input/coca-corpus-sample-as-single-file/"
output_folder <- "/kaggle/working/"
#======================================================
# Required libraries, load yourselves!
pkgs <- c("tidyverse", "tidytext", "tm", "tokenizers", "rword2vec", "readr", "kableExtra")
for (pkg in pkgs) {
  if (! (pkg %in% rownames(installed.packages()))){install.packages(pkg)}
  library(pkg, character.only = TRUE)
}
rm(pkgs, pkg)
gc()
#======================================================
# Define functions
# Bulletin lines getter
get_bulletinLines <- function(file_name, dir, quietly = F){
  if(!quietly){print(file_name)}
  bulletin_lines <- readLines(paste0(dir, file_name))
  bulletin_lines <- as.character(unlist(bulletin_lines))
  bulletin_lines <- str_trim(bulletin_lines)
  ind_rm <- which(bulletin_lines == "")
  if(length(ind_rm) != 0){
    bulletin_lines <- bulletin_lines[-ind_rm]
    
  }
  return(bulletin_lines)
}

get_jobTitle_fromBulletin <- function(bulletin_lines, gsub_pattern){
  #------------------------------
  # This snippet required since some job titles appear on second line
  # of bulletin, with "CAMPUS INTERVIEWS ONLY" taking the first line.
  jobTitle_line <- ifelse(length(grep("CAMPUS INTERVIEWS ONLY", bulletin_lines[1])) == 0,
                          bulletin_lines[1], bulletin_lines[2])
  # The snippet can be removed if job title always appears on first line of bulletin.
  #------------------------------
  str <- gsub(gsub_pattern, "", jobTitle_line)
  jobTitle_from_bulletin <- str_trim(str)
  return(jobTitle_from_bulletin)
}


get_openDate <- function(bulletin_lines, gsub_pattern = "\\b[^0-9-]"){
  ind_openDate <- grep("Open Date", bulletin_lines, ignore.case = T)
  xx <- bulletin_lines[ind_openDate[1]]
  open_date <- gsub(gsub_pattern, "", xx)
  #if(nchar(open_date) > 8){open_date <- substr(open_date, 5, 12)} #"MARINE ENVIRONMENTAL SUPERVISOR" "SENIOR COMPUTER OPERATOR 1428 102017.txt" "SUPERVISING CRIMINALIST 2235 030416.txt" 
  return(open_date)
}



# Job class number from bulletin getter function
get_jobClassNum_fromBulletin <- function(bulletin_lines){
  ind_line <- grep("\\d{4}", bulletin_lines[1:3])
  if(length(ind_line) != 0){
    ind_line <- ind_line[1]
    job_class_num <- gsub(".*(\\d{4}).*", "\\1", bulletin_lines[ind_line])
    jobClassNum_from_bulletin <- str_trim(job_class_num)
  }else{
    jobClassNum_from_bulletin <- NA
  }
  return(jobClassNum_from_bulletin)
}


# Bulletin section headings getter
get_sectionHeadings <- function(bulletin_lines,
                                grep_pattern = "^[A-Z :/,\t\\(\\)]+$"){
  ind_headings <- grep(grep_pattern, bulletin_lines)
  section_headings <- bulletin_lines[ind_headings]
  section_headings <- str_trim(section_headings)
  return(section_headings)
}

# Section text getter
get_sectionTxt <- function(section_name, bulletin_lines, section_headings){
  ind_section_start <- grep(section_name, bulletin_lines) + 1
  if(length(ind_section_start) != 0){
    str_section_end <- section_headings[grep(section_name, section_headings) + 1]
    ind_section_end <- grep(str_section_end, bulletin_lines) - 1
    ind_rm <- which(ind_section_end < ind_section_start)
    if(length(ind_rm) != 0){
      ind_section_end <- ind_section_end[-ind_rm]
    }
    ind_section_end <- ind_section_end[1]
    str_section <- bulletin_lines[ind_section_start:ind_section_end]
    str_section <- paste(str_section, collapse = " ")
  }else{
    str_section <- ""
  }
}

# Convert written numbers to digits
convert_writNum_to_digits <- function(writNum){
  oneToTen <- "(one|two|three|four|five|six|seven|eight|nine|ten)"
  if(length(grep(oneToTen, writNum, ignore.case = T)) != 0){
    if(length(grep("one", writNum, ignore.case = T)) != 0){digitNum <- gsub("one", "1", writNum, ignore.case = T)}
    if(length(grep("two", writNum, ignore.case = T)) != 0){digitNum <- gsub("two", "2", writNum, ignore.case = T)}
    if(length(grep("three", writNum, ignore.case = T)) != 0){digitNum <- gsub("three", "3", writNum, ignore.case = T)}
    if(length(grep("four", writNum, ignore.case = T)) != 0){digitNum <- gsub("four", "4", writNum, ignore.case = T)}
    if(length(grep("five", writNum, ignore.case = T)) != 0){digitNum <- gsub("five", "5", writNum, ignore.case = T)}
    if(length(grep("six", writNum, ignore.case = T)) != 0){digitNum <- gsub("six", "6", writNum, ignore.case = T)}
    if(length(grep("seven", writNum, ignore.case = T)) != 0){digitNum <- gsub("seven", "7", writNum, ignore.case = T)}
    if(length(grep("eight", writNum, ignore.case = T)) != 0){digitNum <- gsub("eight", "8", writNum, ignore.case = T)}
    if(length(grep("nine", writNum, ignore.case = T)) != 0){digitNum <- gsub("nine", "9", writNum, ignore.case = T)}
    if(length(grep("ten", writNum, ignore.case = T)) != 0){digitNum <- gsub("ten", "10", writNum, ignore.case = T)}
    out <- digitNum
    
  }else{
    out <- NA
  }
  return(out)
}


# Get required experience job title (EXP_JOB_CLASS_TITLE)
get_expJobClassTitle <- function(experience_requirement, pattern_jobTitle, pattern_jobTitle_trimPunct, stopWords){
  xx_jobTitle <- gsub(pattern_jobTitle, "\\2", experience_requirement, ignore.case = T)
  if(length(xx_jobTitle) != 0){
    if(!is.na(xx_jobTitle)){
      if(nchar(xx_jobTitle) > 70){
        exp_job_class_title <- NA
      }else{
        # Trim commas etc. from required experience job title
        xx_jobTitle <- gsub(pattern_jobTitle_trimPunct, "\\1", xx_jobTitle, ignore.case = T)
        # Trim stop words from required experience job title
        xx_jobTitle <- unlist(str_split(xx_jobTitle, " "))
        xx_jobTitle <- xx_jobTitle[!(xx_jobTitle %in% stopWords)]
        xx_jobTitle <- xx_jobTitle[!(unlist(str_split(xx_jobTitle, " ")) %in% stopWords)]
        exp_job_class_title <- paste(xx_jobTitle, collapse = " ")
      }
    }
  }
  return(exp_job_class_title)
}


# Get specific required experience types (EXP_JOB_CLASS_FUNCTION)
get_jobClassFunction <- function(experience_requirement, pattern_spec_exp){
  if(length(grep("years of which", experience_requirement)) != 0){
    oneToTen <- "(one|two|three|four|five|six|seven|eight|nine|ten)"
    spec_exp_clause_start <- paste(oneToTen, "years of which")
    jobClassFn_length <- gsub(paste0("^.*", spec_exp_clause_start, ".*$"), "\\1", experience_requirement)
    xx_this_exp_req <- unlist(str_split(experience_requirement, spec_exp_clause_start))
    this_exp_req <- xx_this_exp_req[1]
    jobClassFn <- paste(xx_this_exp_req[-1], collapse = " ")
  }else{
    jobClassFn <- gsub(pattern_spec_exp, "\\1", experience_requirement)
    jobClassFn_length <- NA
  }
  return(c(jobClassFn, jobClassFn_length))
}


# Get required experience length (EXP_LENGTH)
get_expLength <- function(experience_requirement, pattern_num_before_exp, pattern_moYr_before_exp){
  exp_length_writ <- gsub(pattern_num_before_exp, "\\1", experience_requirement, ignore.case = T)
  exp_length <- convert_writNum_to_digits(exp_length_writ)
  if(!is.na(exp_length)){
    if(nchar(exp_length) > 70){
      exp_length <- NA
    }else{
      # If experience length expressed in months, divide by 12
      exp_length <- as.numeric(exp_length)
      exp_length_units <- gsub(pattern_moYr_before_exp, "\\1", experience_requirement, ignore.case = T)
      if(exp_length_units == "months"){exp_length <- exp_length / 12}
    }
  }
  return(exp_length)
}

#================================================
# Required Job Experience Module:
get_reqJobExpInfo <- function(str_Requirements,
                              pattern_num_before_exp = "^.*(one|two|three|four|five|six|seven|eight|nine|ten)\\W(?:\\w+\\W+){1,6}?(full-time|full time|part-time|part time|experience)\\b.*$",
                              pattern_spec_exp = "^.*?\\b[a-z]\\.(.*)$",
                              pattern_moYr_before_exp = "^.*\\b(months|years)\\W+(?:\\w+\\W+){1,6}?(full-time|full time|part-time|part time|experience)\\b.*$",
                              pattern_fullPartTime_before_exp = "^.*\\b(full-time|full time|part-time|part time)\\W+(?:\\w+\\W+){1,6}?experience\\b.*$",
                              pattern_jobTitle = "^.*\\b(as a|as an|at the level of|experience in)\\W+(\\w+\\W+\\w+\\W+\\w+\\W+\\w+).*$",
                              pattern_jobTitle_trimPunct = "(^.*)[,;\\.].*$"){
  
  #-------------------
  #n_reqs <- str_count(str_Requirements, "\\b[0-9]\\.") + 1
  oneToTen = "(one|two|three|four|five|six|seven|eight|nine|ten)"
  stopWords = stopwords("en")
  #-------------------
  # Split the requirements section into its numbered sections.
  req_vec <- unlist(strsplit(str_Requirements, "[0-9]\\."))
  ind_rm <- which(req_vec == "")
  if(length(ind_rm) != 0){req_vec <- req_vec[-ind_rm]}
  #===================================================
  # EXTRACT REQUIRED JOB EXPERIENCE INFO
  # (EXP_LENGTH, FULL_TIME_PART_TIME, EXP_JOB_CLASS_TITLE,
  # EXP_JOB_CLASS_FUNCTION)
  # Retain only the requirements having to do with job experience
  ind_jobExp <- grep("experience", req_vec)
  req_jobExp_vec <- req_vec[ind_jobExp]
  # Go through each job experience requirement and pull out req'd exp info
  if(length(ind_jobExp) != 0){
    exp_job_class_title_vec <- c()
    exp_length_vec <- c()
    full_time_part_time_vec <- c()
    jobClassFn_vec <- c()
    jobClassFn_length_vec <- c()
    for(j in 1:length(ind_jobExp)){
      this_jobExp_req <- req_jobExp_vec[j]
      #--------------------------------------------------
      # Required experience job title (EXP_JOB_CLASS_TITLE)
      exp_job_class_title_vec[j] <- get_expJobClassTitle(this_jobExp_req, pattern_jobTitle, pattern_jobTitle_trimPunct, stopWords)
      #--------------------------------------------------
      # Get required experience length (EXP_LENGTH)
      exp_length_vec[j] <- get_expLength(this_jobExp_req, pattern_num_before_exp, pattern_moYr_before_exp)
      #--------------------------------------------------
      # Get full time or part time (FULL_TIME_PART_TIME)
      full_time_part_time <- gsub(pattern_fullPartTime_before_exp, "\\1", this_jobExp_req, ignore.case = T)
      if(nchar(full_time_part_time) > 50){full_time_part_time_vec[j] <- NA}else{full_time_part_time_vec[j] <- full_time_part_time}
      #--------------------------------------------------
      # Get specific types of required experience (JOB_CLASS_FUNCTION)
      n_subreqs <- str_count(this_jobExp_req, "\\b[a-z]\\.") + 1
      if(n_subreqs > 1){
        out <- get_jobClassFunction(this_jobExp_req, pattern_spec_exp)
        jobClassFn_vec[j] <- out[1]
        jobClassFn_length_vec[j] <- out[2]
      }else{
        jobClassFn_vec[j] <- NA
        jobClassFn_length_vec[j] <- NA
      }
      #--------------------------------------------------
      
    }
    
    df_jobExperience <- data.frame(
      EXP_LENGTH = paste(exp_length_vec, collapse = "|"),
      FULL_TIME_PART_TIME = paste(full_time_part_time_vec, collapse = "|"),
      EXP_JOB_CLASS_TITLE = paste(exp_job_class_title_vec, collapse = "|"),
      EXP_JOB_CLASS_FUNCTION = paste(jobClassFn_vec, collapse = "|"))
  }else{
    df_jobExperience <- data.frame(
      EXP_LENGTH = NA,
      FULL_TIME_PART_TIME = NA,
      EXP_JOB_CLASS_TITLE = NA,
      EXP_JOB_CLASS_FUNCTION = NA)
  }
  return(df_jobExperience)
}


#================================================
# Comprehensive salary info getter
#str_annSalary <- str_annSalary_GEN
get_salaries <- function(str_annSalary){
  str_annSalary <- gsub(", ", ",", str_annSalary) # WATER UTILITY SUPERINTENDENT
  str_annSalary <- gsub(" ,", ",", str_annSalary)
  str_annSalary <- gsub(",\\$", " \\$", str_annSalary)
  #--
  # Get all salary scales and/or flat-rated salaries from salary text
  annSalaries_raw <- removePunctuation(str_annSalary) # Remove all but numbers and "to" #\\(.*\\),;\\$
  annSalaries_raw <- gsub("\\b55\\b", "", annSalaries_raw) # Remove "5.5% bonus" ("ADVANCE PRACTICE PROVIDER CORRECTIONAL CARE 2325 020808 REV 111214.txt", "HELICOPTER MECHANIC 3742 072206 REV 020818.txt", "PRINCIPAL DETENTION OFFICER 3215 101218.txt")
  salary_scales_vec <- unlist(str_extract_all(annSalaries_raw, "[0-9]+ to [0-9]+")) # Keep only phrases of type "[some number] to [some number]"
  salaries_flat <- str_trim(gsub("[0-9]+ to [0-9]+|[a-zA-z]+", "", annSalaries_raw)) # Keep all except phrases of type "[some number] to [some number]"
  salaries_flat <- unlist(str_split(salaries_flat, " "))
  ind_rm <- which(salaries_flat == "")
  if(length(ind_rm) != 0){salaries_flat <- salaries_flat[-ind_rm]}
  #----------------------------------------------
  # The following snippet is needed to handle edge case "BOILERMAKER 3735 110714.txt"
  # which has a 5 figure salary expressed to 2 decimal places.
  # The snippet removes the 2 decimal places.
  if(!is.null(salaries_flat)){
    if(length(salaries_flat) != 0){
      if(nchar(salaries_flat) == 7){
        salaries_flat <- gsub(".{2}$", "", salaries_flat)
      }
    }
  }
  # The above snippet can be removed if all input files have salaries
  # rounded to nearest dollar (no decimal places).
  #----------------------------------------------
  # Number of flat rated salaries and salary scales
  #annSalaries_all <- as.numeric(annSalaries_all)
  n_salary_scales <- length(salary_scales_vec)
  n_flat_rated_salaries <- length(salaries_flat)
  if(n_salary_scales + n_flat_rated_salaries > 0){
    # Deal with salary scales
    if(n_salary_scales > 0){
      ind_even <- 2 * n_salary_scales
      ind_odd <- 2 * n_salary_scales - 1
      salary_scale_bounds <- sapply(salary_scales_vec, str_split, "to")
      salary_scale_bounds <- as.numeric(unlist(salary_scale_bounds))
      # Salary scale lower bounds
      salScale_loBound_numeric <- salary_scale_bounds[seq(1, ind_odd, 2)]
      salScale_loBound <- paste(as.character(salScale_loBound_numeric), collapse = ", ")
      # Salary scale upper bounds
      salScale_upBound_numeric <- salary_scale_bounds[seq(2, ind_even, 2)]
      salScale_upBound <- paste(as.character(salScale_upBound_numeric), collapse = ", ")
      # Difference between upper and lower bounds
      salScaleDiff <- diff(salary_scale_bounds)[seq(1, ind_odd, 2)]
      salScaleDiff <- paste(as.character(salScaleDiff), collapse = ", ")
      # Convert salary scales list to string for output
      salary_scales <- as.character(paste(salary_scales_vec, collapse = ", "))
    }else{
      salary_scales <- NA
      salScale_loBound <- NA
      salScale_upBound <- NA
      salScale_loBound_numeric <- NA
      salScale_upBound_numeric <- NA
      salScaleDiff <- NA
    }
    # Deal with flat-rated salaries
    if(n_flat_rated_salaries > 0){
      flat_salaries_numeric <- as.numeric(salaries_flat)
      flat_salaries <- paste(salaries_flat, collapse = ", ")
    }else{
      flat_salaries_numeric <- NA
      flat_salaries <- NA
    }
    # Calculate min, max over both flat rates and scales
    salary_min <- min(c(flat_salaries_numeric, salScale_loBound_numeric), na.rm = T)
    salary_max <- max(c(flat_salaries_numeric, salScale_upBound_numeric), na.rm = T)
    if(n_salary_scales > 0){
      ENTRY_SALARY <- salary_scales_vec[1]
      
    }else{
      ENTRY_SALARY <- salaries_flat[1]
    }
    #----------------------------------------------
  }else{
    salary_scales <- NA
    flat_salaries <- NA
    salary_min <- NA
    salary_max <- NA
    salScale_loBound <- NA
    salScale_upBound <- NA
    salScaleDiff <- NA
    ENTRY_SALARY <- NA
  }
  #----------------------------------------------
  df_salary_info <- data.frame(ENTRY_SALARY,
                               n_salary_scales,
                               n_flat_rated_salaries,
                               salary_scales,
                               flat_salaries,
                               salary_min,
                               salary_max,
                               salScale_loBound,
                               salScale_upBound,
                               salScaleDiff)
  
  return(df_salary_info)
}


#======================================================
# Bulletin parser -- puts all of the above together
#file_name <- file_names[1]
#file_name <- "ADMINISTRATIVE CLERK 1358 033018 (2).txt"
#file_name <- "AIR CONDITIONING MECHANIC 3774 041417.txt"
#file_name <- "AIRPORT POLICE SPECIALIST 3236 063017 (2).txt"
#file_name <- "SENIOR AUTOMOTIVE SUPERVISOR 3716 112015.txt"                    
#file_name <- "SENIOR BUILDING OPERATING ENGINEER 5925 011615 (1).txt"          
#file_name <- "SENIOR CARPENTER  3345 081117 REV 082417.txt"                    
#file_name <- "SENIOR COMMUNICATIONS ELECTRICIAN 3638 030317 (1).txt"  
#file_name <- "ADVANCE PRACTICE PROVIDER CORRECTIONAL CARE 2325 020808 REV 111214.txt"
#file_name <- "HELICOPTER MECHANIC 3742 072206 REV 020818.txt"                        
#file_name <- "POLICE COMMANDER 2251 092917.txt"
#file_path <- bulletins_folder
parse_bulletin <- function(file_name, file_path){
  print(file_name)
  bulletin_lines <- get_bulletinLines(file_name, file_path, quietly = T)
  section_headings <- get_sectionHeadings(bulletin_lines)
  #---------------------------------------------
  # Job class title
  # job_class_title <- bulletin_lines[1]
  # job_class_title <- gsub("[0-9\\(\\)\\.txt]|REV|REVISED|[a-z]", "", file_name)
  # job_class_title <- str_trim(job_class_title)
  job_title <- get_jobTitle_fromBulletin(bulletin_lines, gsub_pattern = "\\'|\\s*\\([^\\)]+\\)")
  #---------------------------------------------
  # Job class number (taken from inside file)
  # job_class_no <- bulletin_lines[grep("Class code", bulletin_lines, ignore.case = T)[1]]
  # job_class_no <- gsub("[^0-9]", "", job_class_no)
  # if(nchar(job_class_no) > 4){job_class_no <- substr(job_class_no, 1, 4)} #"MARINE ENVIRONMENTAL SUPERVISOR" "SENIOR COMPUTER OPERATOR 1428 102017.bulletin_lines" "SUPERVISING CRIMINALIST 2235 030416.txt" 
  job_class_num <- get_jobClassNum_fromBulletin(bulletin_lines)
  #---------------------------------------------
  # Opening date
  # ind_openDate <- grep("Open Date", bulletin_lines, ignore.case = T)
  # xx <- bulletin_lines[ind_openDate[1]]
  # open_date <- gsub("\\b[^0-9-]", "", xx)
  # if(nchar(open_date) > 8){open_date <- substr(open_date, 5, 12)} #"MARINE ENVIRONMENTAL SUPERVISOR" "SENIOR COMPUTER OPERATOR 1428 102017.txt" "SUPERVISING CRIMINALIST 2235 030416.txt" 
  open_date <- get_openDate(bulletin_lines)
  #---------------------------------------------
  # Get main text sections and their word, sentence counts, and words per sentence
  # Requirements
  str_Requirements <- get_sectionTxt(section_name = "REQUIREMENT",
                                     bulletin_lines,
                                     section_headings)
  num_words_Requirements <- str_count(str_Requirements, "\\w+")
  num_sents_Requirements <- length(unlist(tokenize_sentences(str_Requirements)))
  words_per_sent_Requirements <- num_words_Requirements / num_sents_Requirements
  #---------
  # Duties
  str_Duties <- get_sectionTxt(section_name = "DUTIES",
                               bulletin_lines,
                               section_headings)
  num_words_Duties <- str_count(str_Duties, "\\w+")
  num_sents_Duties <- length(unlist(tokenize_sentences(str_Duties)))
  words_per_sent_Duties <- num_words_Duties / num_sents_Duties
  #---------
  # Process Notes
  str_procNotes <- get_sectionTxt(section_name = "PROCESS NOTES",
                                  bulletin_lines,
                                  section_headings)
  str_procNotes <- gsub("r\xe9sum\xe9", "resume", str_procNotes)
  num_words_ProcNotes <- str_count(str_Duties, "\\w+")
  num_sents_ProcNotes <- length(unlist(tokenize_sentences(str_procNotes)))
  words_per_sent_ProcNotes <- num_words_ProcNotes / num_sents_ProcNotes
  #---------
  # Selection Process
  str_selectProc <- get_sectionTxt(section_name = "SELECTION PROCESS",
                                   bulletin_lines,
                                   section_headings)
  str_selectProc <- gsub("r\xe9sum\xe9", "resume", str_selectProc)
  num_words_SelectProc <- str_count(str_selectProc, "\\w+")
  num_sents_SelectProc <- length(unlist(tokenize_sentences(str_selectProc)))
  words_per_sent_SelectProc <- num_words_SelectProc / num_sents_SelectProc
  #---------
  # Get word, sentence count, words per sentence, for all retained text
  num_words_total <- sum(c(num_words_Requirements, num_words_Duties, 
                           num_words_ProcNotes, num_words_SelectProc), na.rm = T)
  num_sents_total <- sum(c(num_sents_Requirements, num_sents_Duties,
                           num_sents_ProcNotes, num_sents_SelectProc), na.rm = T)
  words_per_sent_total <- num_words_total / num_sents_total
  #---------------------------------------------
  # Get Required Job Experience Info from the Requirements Text
  df_jobExpInfo <- get_reqJobExpInfo(str_Requirements,
                                     pattern_num_before_exp = "^.*(one|two|three|four|five|six|seven|eight|nine|ten)\\W(?:\\w+\\W+){1,6}?(full-time|full time|part-time|part time|experience)\\b.*$",
                                     pattern_spec_exp = "^.*?\\b[a-z]\\.(.*)$",
                                     pattern_moYr_before_exp = "^.*\\b(months|years)\\W+(?:\\w+\\W+){1,6}?(full-time|full time|part-time|part time|experience)\\b.*$",
                                     pattern_fullPartTime_before_exp = "^.*\\b(full-time|full time|part-time|part time)\\W+(?:\\w+\\W+){1,6}?experience\\b.*$",
                                     pattern_jobTitle = "^.*\\b(as a|as an|at the level of|experience in)\\W+(\\w+\\W+\\w+\\W+\\w+\\W+\\w+).*$",
                                     pattern_jobTitle_trimPunct = "(^.*)[,;\\.].*$")
  #---------------------------------------------
  # DRIVERS_LICENCE_REQ
  license_pattern_P <- "may require a valid California driver's license"
  license_pattern_R <- "driver's license is required"
  ind_license_P <- grep(license_pattern_R, bulletin_lines, ignore.case = T)
  ind_license_R <- grep(license_pattern_R, bulletin_lines, ignore.case = T)
  if(length(ind_license_P) != 0){drivers_license_req <- "P"}
  if(length(ind_license_R) != 0){drivers_license_req <- "R"}
  if(length(ind_license_R) == 0 & length(ind_license_P) == 0){drivers_license_req <- NA}
  #---------------------------------------------
  # EDUCATION_YEARS, SCHOOL_TYPE
  educYrs_pattern <- "four-year|Bachelor's degree"
  educSchool_pattern <- "college|university"
  ind_educYrs <- grep(educYrs_pattern, bulletin_lines, ignore.case = T)
  ind_educSchool <- grep(educSchool_pattern, bulletin_lines, ignore.case = T)
  if(length(ind_educYrs) != 0){education_years <- 4}else{education_years <- NA}
  if(length(ind_educSchool) != 0){
  school_type <- educSchool_pattern
  }else{
        educSchool_pattern <- "apprenticeship"
    ind_educSchool <- grep(educSchool_pattern, bulletin_lines, ignore.case = T)
    if(length(ind_educSchool) != 0){
      school_type <- educSchool_pattern
    }else{
      educSchool_pattern <- "high school"
      ind_educSchool <- grep(educSchool_pattern, bulletin_lines, ignore.case = T)
      if(length(ind_educSchool) != 0){
        school_type <- educSchool_pattern
      }else{
          school_type <- NA
      }
  }
  }
  #---------------------------------------------
  # EXAM_TYPE
  INT_DEPT_PROM <- ifelse(sum(str_detect(section_headings, "INTERDEPARTMENTAL PROMOTIONAL|INTERDEPARMENTAL PROMOTIONAL")) == 1, 1, 0)
  DEPT_PROM <- ifelse(sum(str_detect(section_headings, "DEPARTMENTAL PROMOTIONAL|DEPARMENTAL PROMOTIONAL")) == 1, 1, 0)
  OPEN <- ifelse(sum(str_detect(section_headings, "OPEN COMPETITIVE")) == 1, 1, 0)
  OPEN_INT_PROM <- ifelse(INT_DEPT_PROM == 1 & OPEN == 1, 1, 0)
  if(DEPT_PROM == 1){
    exam_type <- "DEPT_PROM"
  }else{
    if(OPEN_INT_PROM == 1){
      exam_type <- "OPEN_INT_PROM"
    }else{
      if(INT_DEPT_PROM == 1){exam_type <- "INT_DEPT_PROM"}
      if(OPEN == 1){exam_type <- "OPEN"}
    }
  }
  #--
  # Salary
  # Get salary text
  salary_section_identifier <- "ANNUAL SALARY|ANNUALSALARY"
  ind_annSalary_start <- grep(salary_section_identifier, bulletin_lines) + 1
  str_annSalary_end <- section_headings[grep(salary_section_identifier, section_headings) + 1]
  ind_annSalary_end <- grep(str_annSalary_end, bulletin_lines) - 1
  ind_rm <- which(ind_annSalary_end < ind_annSalary_start)
  if(length(ind_rm) != 0){ind_annSalary_end <- ind_annSalary_end[-ind_rm]}
  ind_annSalary_end <- ind_annSalary_end[1]
  str_annSalary <- bulletin_lines[ind_annSalary_start:ind_annSalary_end]
  str_annSalary <- paste(str_annSalary, collapse = " ")
  # Split into GEN and DWP salary text
  str_annSalary_vec <- unlist(str_split(str_annSalary, "Power"))
  str_annSalary_GEN <- str_annSalary_vec[1]
  str_annSalary_DWP <- str_annSalary_vec[2]
  if(is.na(str_annSalary_GEN)){str_annSalary_GEN <- NULL}
  if(is.na(str_annSalary_DWP)){str_annSalary_DWP <- NULL}
  #--
  # Get GEN salary info
  df_annSalary_GEN <- get_salaries(str_annSalary_GEN)
  colnames(df_annSalary_GEN) <- paste0(colnames(df_annSalary_GEN), "_GEN")
  # Get DWP salary info
  df_annSalary_DWP <- get_salaries(str_annSalary_DWP)
  colnames(df_annSalary_DWP) <- paste0(colnames(df_annSalary_DWP), "_DWP")
  # Get salary min, max, mean, and coefficient of variation across both GEN and DWP
  # mean and cv taken over lower bounds of salary scales
  df_annSalary <- cbind(df_annSalary_DWP, df_annSalary_GEN)
  rm(df_annSalary_DWP, df_annSalary_GEN)
  if(df_annSalary$n_salary_scales_DWP == 0 & 
     df_annSalary$n_salary_scales_GEN == 0 &
     df_annSalary$n_flat_rated_salaries_DWP == 0 & 
     df_annSalary$n_flat_rated_salaries_GEN == 0){
    # If no salary reported at all
    # (one case: "AIRPORT POLICE SPECIALIST 3236 063017 (2).txt")
    salary_min_DWPandGEN <- NA
    salary_max_DWPandGEN <- NA
    salary_meanLo_DWPandGEN <- NA
    salary_cvLo_DWPandGEN <- NA
    salScaleDiff_min_DWPandGEN <- NA
    salScaleDiff_max_DWPandGEN <- NA
    salScaleDiff_mean_DWPandGEN <- NA
    salScaleDiff_cv_DWPandGEN <- NA
  }else{
    salary_min_DWPandGEN <- min(df_annSalary$salary_min_DWP, df_annSalary$salary_min_GEN, na.rm = T)
    salary_max_DWPandGEN <- max(df_annSalary$salary_max_DWP, df_annSalary$salary_max_GEN, na.rm = T)
    if(df_annSalary$n_salary_scales_DWP + df_annSalary$n_salary_scales_GEN > 1){
      salScale_loBound_DWP <- as.character(df_annSalary$salScale_loBound_DWP)
      salScale_loBound_GEN <- as.character(df_annSalary$salScale_loBound_GEN)
      if(is.na(salScale_loBound_DWP)){salScale_loBound_DWP <- NULL}
      if(is.na(salScale_loBound_GEN)){salScale_loBound_GEN <- NULL}
      salScale_loBound_DWPandGEN <- paste(c(salScale_loBound_DWP, salScale_loBound_GEN), collapse = ", ")
      #salScale_upBound_DWPandGEN <- paste(df_annSalary$salScale_upBound_DWP, df_annSalary$salScale_upBound_GEN, collapse = ", ")
      loBound_vec <- as.numeric(unlist(str_split(salScale_loBound_DWPandGEN, ", ")))
      salary_meanLo_DWPandGEN <- mean(c(loBound_vec, salary_min_DWPandGEN), na.rm = T)
      salary_cvLo_DWPandGEN <- sd(c(loBound_vec, salary_min_DWPandGEN), na.rm = T) / salary_meanLo_DWPandGEN
      # Get salary scale difference min, max, mean, and coefficient of variation
      salScaleDiff_DWP <- as.character(df_annSalary$salScaleDiff_DWP)
      salScaleDiff_GEN <- as.character(df_annSalary$salScaleDiff_GEN)
      if(is.na(salScaleDiff_DWP)){salScaleDiff_DWP <- NULL}
      if(is.na(salScaleDiff_GEN)){salScaleDiff_GEN <- NULL}
      salScaleDiff_DWPandGEN <- paste(c(salScaleDiff_DWP, salScaleDiff_GEN), collapse = ", ")
      salScaleDiff_DWPandGEN <- unlist(str_split(salScaleDiff_DWPandGEN, ", "))
      xx <- as.numeric(salScaleDiff_DWPandGEN)
      salScaleDiff_min_DWPandGEN <- min(xx, na.rm = T)
      salScaleDiff_max_DWPandGEN <- max(xx, na.rm = T)
      salScaleDiff_mean_DWPandGEN <- mean(xx, na.rm = T)
      salScaleDiff_cv_DWPandGEN <- sd(xx, na.rm = T) / salScaleDiff_mean_DWPandGEN
    }else{
      salary_meanLo_DWPandGEN <- salary_min_DWPandGEN
      salary_cvLo_DWPandGEN <- NA
      salScaleDiff_min_DWPandGEN <- NA
      salScaleDiff_max_DWPandGEN <- NA
      salScaleDiff_mean_DWPandGEN <- NA
      salScaleDiff_cv_DWPandGEN <- NA
    }
  }
  #
  df_annSalary_DWPandGEN <- data.frame(salary_min_DWPandGEN,
                                       salary_max_DWPandGEN,
                                       salary_meanLo_DWPandGEN,
                                       salary_cvLo_DWPandGEN,
                                       salScaleDiff_min_DWPandGEN,
                                       salScaleDiff_max_DWPandGEN,
                                       salScaleDiff_mean_DWPandGEN,
                                       salScaleDiff_cv_DWPandGEN
  )
  df_annSalary <- cbind(df_annSalary, df_annSalary_DWPandGEN)
  rm(df_annSalary_DWPandGEN)
  #=============================================
  df_out <- data.frame(FILE_NAME = file_name,
                       JOB_CLASS_TITLE = job_title,
                       JOB_CLASS_NO = job_class_num,
                       JOB_DUTIES = str_Duties,
                       EXAM_TYPE = exam_type,
                       OPEN_DATE = open_date,
                       DRIVERS_LICENSE_REQ = drivers_license_req,
                       EDUCATION_YEARS = education_years,
                       SCHOOL_TYPE = school_type,
                       EDUCATION_MAJOR = "",
                       EXP_JOB_CLASS_ALT_RESP = "",
                       COURSE_COUNT = "",
                       COURSE_LENGTH = "",
                       COURSE_SUBJECT = "",
                       MISC_COURSE_DETAILS = "",
                       DRIV_LIC_TYPE = "",
                       ADDTL_LIC = "",
                       num_words_total = num_words_total,
                       num_sents_total = num_sents_total,
                       words_per_sent_total = words_per_sent_total,
                       num_words_requirements = num_words_Requirements,
                       num_sents_Requirements = num_sents_Requirements,
                       words_per_sent_Requirements = words_per_sent_Requirements,
                       num_words_Duties = num_words_Duties,
                       num_sents_Duties = num_sents_Duties,
                       words_per_sent_Duties = words_per_sent_Duties,
                       num_words_ProcNotes = num_words_ProcNotes,
                       num_sents_ProcNotes = num_sents_ProcNotes,
                       words_per_sent_ProcNotes = words_per_sent_ProcNotes,
                       num_words_SelectProc = num_words_SelectProc,
                       num_sents_SelectProc = num_sents_SelectProc,
                       words_per_sent_SelectProc = words_per_sent_SelectProc,
                       Requirements_Text = str_Requirements,
                       Duties_Text = str_Duties,
                       Process_Notes_Text = str_procNotes,
                       Selection_Process_Text = str_selectProc)
  list_df_out <- list(df_out, df_jobExpInfo, df_annSalary)
  df_out <- as.data.frame(do.call(cbind, list_df_out))
  # Order columns so that host's variables come first
  these_first <- c("FILE_NAME",
                   "JOB_CLASS_TITLE",
                   "JOB_CLASS_NO",
                   "JOB_DUTIES",
                   "EDUCATION_YEARS",
                   "SCHOOL_TYPE",
                   "EDUCATION_MAJOR",
                   "EXP_LENGTH",
                   "FULL_TIME_PART_TIME",
                   "EXP_JOB_CLASS_TITLE",
                   "EXP_JOB_CLASS_ALT_RESP",
                   "EXP_JOB_CLASS_FUNCTION",
                   "COURSE_COUNT",
                   "COURSE_LENGTH",
                   "COURSE_SUBJECT",
                   "MISC_COURSE_DETAILS",
                   "DRIVERS_LICENSE_REQ",
                   "DRIV_LIC_TYPE",
                   "ADDTL_LIC",
                   "EXAM_TYPE",
                   "ENTRY_SALARY_GEN",
                   "ENTRY_SALARY_DWP",
                   "OPEN_DATE")
  then_these <- setdiff(colnames(df_out), these_first)
  df_out <- df_out[, c(these_first, then_these)]
  return(df_out)
}

```
# 1 Data Cleaning
Compare job titles in file names with job titles found inside the file. Look for mislabeled or duplicate files.

```{r}

file_names <- list.files(bulletins_folder)
gsubPattern_in_fileName <- "[0-9\\._]|\\b[A-Z][a-z]+|[a-z]|\\s*\\([^\\)]+\\)|.txt|\\bREV\\b|REVISED|FINAL|\\bDRAFT\\b|TRACK CHANGES"
gsubPattern_in_txt <- "\\'|\\s*\\([^\\)]+\\)"
jobTitle_from_fileName <- c()
jobTitle_from_txt <- c()
for(i in 1:length(file_names)){
  file_name <- file_names[i]
  str <- gsub(gsubPattern_in_fileName, "", file_name)
  jobTitle_from_fileName[i] <- str_trim(str)
  #----------------
  bulletin_lines <- get_bulletinLines(file_name, bulletins_folder, quietly = T)
  jobTitle_from_txt[i] <- get_jobTitle_fromBulletin(bulletin_lines, gsubPattern_in_txt)
  
}

ind_dup <- which(duplicated(jobTitle_from_fileName))
duplicated_jobTitles_fileName <- jobTitle_from_fileName[ind_dup]
kable(duplicated_jobTitles_fileName)

```
So, there are two job titles that are found in more than one file name.

```{r}
ind_dup <- which(duplicated(jobTitle_from_txt))
duplicated_jobTitles_txt <- jobTitle_from_txt[ind_dup]
kable(duplicated_jobTitles_txt)

```
So, there are five job titles that are found in more than one file. Two of these are the same ones we just saw repeated in the file names. Let's compare the duplicates' job titles as displayed in the file names vs. as displayed inside the files:

```{r}
df_look <- data.frame(jobTitle_from_fileName, jobTitle_from_txt)
kable(df_look[which(jobTitle_from_txt %in% duplicated_jobTitles_txt), ])

```
There is a mismatch between job titles as found in the file names vs. as found inside the files for three jobs:

"ANIMAL CARE TECHNICIAN SUPERVISOR"
"SENIOR EXAMINER OF QUESTIONED DOCUMENTS"
"WASTEWATER COLLECTION SUPERVISOR"

Upon inspection of the file contents, it turns out that these are mislabeled duplicates of, respectively, the "WATER TREATMENT OFFICER", "WATER UTILITY SUPERINTENDENT", and "WASTEWATER TREATMENT OPERATOR" bulletins. Let's go ahead and delete these.

```{r}

remove_these <- c("ANIMAL CARE TECHNICIAN SUPERVISOR",
                  "SENIOR EXAMINER OF QUESTIONED DOCUMENTS",
                  "WASTEWATER COLLECTION SUPERVISOR")
ind_rm <- which(jobTitle_from_fileName %in% remove_these)
file_names <- file_names[-ind_rm]

```
There are also two duplicates without a mismatch between the file name and file content versions of the job title:

"CHIEF CLERK POLICE"
"SENIOR UTILITY SERVICES SPECIALIST"

Upon inspection of the bulletin folder, there are two versions of each of these bulletins, one being more recent than the other. The job class number also differs between the two versions. I assume the host prefers to keep the more recent versions. To find out which is more recent I have to extract the opening dates.

```{r}

ind_find <- grep("CHIEF CLERK POLICE|SENIOR UTILITY SERVICES SPECIALIST", file_names)
dup_files <- file_names[ind_find]
openDate_vec <- c()
for(i in 1:length(dup_files)){
  dupFile_name <- dup_files[i]
  bulletin_lines <- get_bulletinLines(dupFile_name, bulletins_folder, quietly = T)
  openDate_vec[i] <- get_openDate(bulletin_lines)
  
}
df_look <- data.frame(dup_file_name = dup_files, open_date = openDate_vec)
kable(df_look)

```
So, in both cases, there is a bulletin with a 2015 open date and one with a 2018 open date. Assuming the more recent version is more valid, let's delete the 2015 versions.

```{r}

dupFiles_old <- as.character(df_look$dup_file_name[c(1, 4)])
ind_rm <- which(file_names %in% dupFiles_old)
file_names <- file_names[-ind_rm]
```
In the last two cases, there were issues with the job class number. This motivates a closer look at job class numbers in all bulletins.

In analysis outside this kernel I found that the job class number occurs in the second line of most bulletins. However, there are several edge cases where it occurs in the first or third line, and sometimes it is repeated much farther down in the bulletin. I design my "get job class number" function accordingly.

Get job class numbers from all file names and file contents to check for duplicates and disagreement between number in file name and number in contents.

```{r}

jobClassNum_from_fileName <- c()
jobClassNum_from_txt <- c()
for(i in 1:length(file_names)){
  file_name <- file_names[i]
  #--------------
  jobClassNum_from_fileName[i] <- gsub(".*\\s(\\d{4})\\s.*", "\\1", file_name)
  #--------------
  bulletin_lines <- get_bulletinLines(file_name, bulletins_folder, quietly = T)
  jobClassNum_from_txt[i] <- get_jobClassNum_fromBulletin(bulletin_lines)
}
ind_dup <- which(duplicated(jobClassNum_from_fileName))
duplicated_jobClassNum_fileName <- jobClassNum_from_fileName[ind_dup]
kable(duplicated_jobClassNum_fileName)
kable(file_names[ind_dup])

```
So, there is one job class number, "7260", that is duplicated in the file names. Let's see both of the files that have this job class number in their file name.

```{r}

ind_look <- grep("7260", file_names)
df_look <- data.frame(file_name = file_names[ind_look],
                      jobClassNum_from_fileName = jobClassNum_from_fileName[ind_look],
                      jobClassNum_from_txt = jobClassNum_from_txt[ind_look])
kable(df_look)
```
The AIRPORT MANAGER job class number in its file name agrees with its number in the file contents. Not so for the PRINCIPAL DEPUTY CONTROLLER. This suggests that the job class number found in the file name of the PRINCIPAL DEPUTY CONTROLLER bulletin is incorrect. It should be the same as the number inside the file (9653).

What about duplicate job class numbers inside the files?

```{r}

ind_dup <- which(duplicated(jobClassNum_from_txt))
duplicated_jobClassNum_txt <- jobClassNum_from_txt[ind_dup]
kable(duplicated_jobClassNum_txt)
kable(file_names[ind_dup])
```
So, there are no file contents with duplicate job class numbers.

I assume from here on out that the job class number found in the bulletin is more valid than the one found in the file name. Hence no cleaning action is required here. Erroneous job class numbers in the file names will not affect my .csv output file.

Nonetheless, it might be of interest to the host to know how many instances of disagreement there are between file name job class numbers and file content job class numbers. So let's pause to check:

```{r}

test <- c()
for(i in 1:length(jobClassNum_from_txt)){
  test[i] <- identical(jobClassNum_from_txt[i], jobClassNum_from_fileName[i])
}
ind_disagree <- which(!test)
df_look <- data.frame(file_name = file_names[ind_disagree],
                      jobClassNum_from_fileName = jobClassNum_from_fileName[ind_disagree],
                      jobClassNum_from_txt = jobClassNum_from_txt[ind_disagree])
kable(df_look)
```
So, there are thirteen bulletins with disagreement between the file name and file content. We see here that in most of these cases this is because no job class number is reported in the file name. (In some cases, my code mistook the open date year as the job class number. By getting the job class number from within the file and not from the file name, I'll avoid this problem.)

Our job class number getter function allows for NA output when no job class number is found. Are there any such instances?

```{r}

ind_NA <- which(is.na(jobClassNum_from_txt))
kable(file_names[ind_NA])
```
Yes, the "Vocational Worker  DEPARTMENT OF PUBLIC WORKS.txt" bulletin has no job class number--neither in the file name nor in the file contents. Upon closer inspection, this bulletin is anomalous in other ways, particularly in its structure and salary information. It is too extreme a case for my code to accomodate, and so I will remove it.

```{r}

special_case <- "Vocational Worker  DEPARTMENT OF PUBLIC WORKS.txt"
ind_rm <- which(file_names == special_case)
file_names <- file_names[-ind_rm]
print(paste("Number of bulletins after cleaning: ", length(file_names)))
```

There are 677 bulletins left after cleaning.

# 2 Retrieving information from the job bulletins, generating the job class .csv output file, and extending the data dictionary

My strategy for retrieving info from the bulletins is to break the bulletin up into sections according to its ALL CAPS headings. These headings vary somewhat across bulletins in spelling, spacing, order of appearance, and such; but there is enough consistency to build a script off of. The functions defined for this task were defined at the end of the Introduction.

The example output file kindly provided by the host indicates that each row should be assigned to each job (sub-)requirement. However, in discussion threads the Kaggle monitor also made clear that contestants "are free to represent the data in whatever way [we] think is most helpful so long as [we] describe our method in the accompanying data dictionary and documentation". After long deliberation, I decided to go with a one-row-per-bulletin output format.

This is because the job requirements information in the bulletins is presented in a way that is often inconsistent with the format proposed by the host.

In particular, the host's format presumes "and/or" logic gates at the end of each (sub-)requirement. This is true for some bulletins, but many bulletins also have and/or logic gates occurring within each (sub-)requirement, effectively creating an additional level of nesting not addressed by the host's output format.

The one-row-per-bulletin format is a stopgap measure pending clarification or homogenization of these bulletin formatting issues. It also facilitates subsequent manipulation of the data for the diversity analysis.

Given this one-row-per-bulletin format, under EXP_JOB_CLASS_TITLE, multiple job experience requirements are separated by "|". The corresponding info under EXP_LENGTH, FULL_TIME_PART_TIME, EXP_JOB_CLASS_TITLE, and JOB_CLASS_FUNCTION, is likewise separated by "|". (The "|" does not necessarily mean "or".) Also, the REQUIREMENT_SET_ID and REQUIREMENT_SUBSET_ID fields serve no purpose in this format and so are not included in the output file.

## 2.1 New fields added to the output file

I have added several new fields to the job class .csv file.

### 2.1.1 New salary fields

Most job bulletins indicate multiple entry salaries, some of which are ranges and some of which are flat-rated. I developed code to parse through this information and:

-separate salary scales from flat rates
-report the min and max of all indicated salaries
-report the mean and coefficient of variation of all indicated salries (in the case of salary scales, only the lower bounds are included in this calculation)
-report the differences between respective salary scale maximums and minimums
-report the mean and coefficient of variation of these differences

This is done for GEN and DWP separately, and also for GEN and DWP together.

### 2.1.2 Word/sentence counts, unique text fields

The bulletins contain a lot of "boiler plate" text that doesn't vary much from one bulletin to another. To reduce noise as much as possible in our diversity analysis farther below, we conduct our analysis only over the text that is unique to each bulletin. The unique text seems to be contained in the REQUIREMENTS, DUTIES, PROCESS NOTES, and, occassionally, SELECTION PROCESS sections of the bulletins. These sections are thus retained in the output file for each bulletin. We also include the word and sentence counts for each of these sections, as well as the total word and sentence counts over all of these sections.

## 2.3 Missing output fields

I ran out of time before I could develop code to extract the following requested information:

EDUCATUION_MAJOR, EXP_JOB_CLASS_ALT_RESP, COURSE_COUNT, COURSE_LENGTH, COURSE_SUBJECT, MISC_COURSE_DETAILS, DRIV_LIC_TYPE, ADDTL_LIC

## 2.4 Here we go

In the following code, I 1) extract the bulletin information into a data frame, 2) write the data frame to a job class information, 3) create a data dictionary for the new fields I created, and 4) import the kaggle data dictionary and append it with the new fields data dictionary.

Most of the work in the lines below is being done by the functions defined at the end of the Introduction above.

```{r}
list_df_outputFile <- map(file_names, parse_bulletin, bulletins_folder)
df_outputFile <- do.call(rbind, list_df_outputFile)
# Change factor to character
for(i in 1:ncol(df_outputFile)){
  #print(class(df_outputFile[, i]))
  if(class(df_outputFile[, i]) == "factor"){
    df_outputFile[, i] <- as.character(df_outputFile[, i])
  }
}
#-----------------------------------------------
# Write job bulletin info output file
output_file_name <- "Job Classes Output.csv"
output_file_path <- paste0(output_folder, output_file_name)
#"/kaggle/working/submission.csv"
write.csv(df_outputFile, output_file_path)

# Add new variables to data dictionary
colnames_dictionary <- c("Field Name", "Annotation Letter", "Description", "Data Type", "Allowable Values", "Accepts Null Values", "Additional Notes")
#============================================================
# New unique text, word/sentence count fields
#============================================================
# New unique text, word/sentence count field names
fieldNames_wordSentCount <- c("num_words_total",
                              "num_sents_total",
                              "words_per_sent_total",
                              "num_words_requirements",
                              "num_sents_Requirements",
                              "words_per_sent_Requirements",
                              "num_words_Duties",
                              "num_sents_Duties",
                              "words_per_sent_Duties",
                              "num_words_ProcNotes",
                              "num_sents_ProcNotes",
                              "words_per_sent_ProcNotes",
                              "num_words_SelectProc",
                              "num_sents_SelectProc",
                              "words_per_sent_SelectProc",
                              "Requirements_Text",
                              "Duties_Text",
                              "Process_Notes_Text",
                              "Selection_Process_Text")
# New unique text, word/sentence count annotation letters
xx1 <- c("Nw", "Ns", "WpS")
xx2 <- c("_Tot", "_Req", "_Dut", "_Proc", "_Sel")
outlist <- list()
for(i in 1:length(xx2)){
  outlist[[i]] <- paste0(xx1, xx2[i])
}
xx3 <- do.call(c, outlist)
annotationLetters_wordSentCount <- c(xx3,
                                     "Req",
                                     "Dut",
                                     "Proc",
                                     "Sel")

# New unique text, word/sentence count variable descriptions
xx1 <- c("Number of words in",
         "Number of sentences in",
         "Words per sentence (Nw / Ns),")
xx2 <- c(" Requirements, Duties, Process Notes, and Selection Process sections",
         " Requirements section",
         " Duties section",
         " Process Notes section",
         " Selection Process section")
outlist <- list()
for(i in 1:length(xx2)){
  outlist[[i]] <- paste0(xx1, xx2[i])
}
xx3 <- do.call(c, outlist)
descriptions_wordSentCount <- c(xx3,
                                "Requirements section text",
                                "Duties section text",
                                "Process Notes section text",
                                "Selection Process section text")

# New unique text, word/sentence count variable data type
dataType_wordSentCount <- c(rep(c("Integer", "Integer", "Float"), length(xx3) / 3), rep("String", 4))
# New unique text, word/sentence count variable allowable values
allowValues_wordSentCount <- c(rep(c("Positive integers", "Positive integers", "Positive real numbers"), length(xx3) / 3), "Requirements section text",
                               "Duties section text",
                               "Process Notes section text",
                               "Selection process section text"
)
# New unique text, word/sentence count variable accepts null?
acceptsNull_wordSentCount <- c(rep("Yes", length(xx3)), rep("No", 4))
df_addNew1 <- data.frame(fieldNames_wordSentCount,
                         annotationLetters_wordSentCount,
                         descriptions_wordSentCount,
                         dataType_wordSentCount,
                         allowValues_wordSentCount,
                         acceptsNull_wordSentCount,
                         AddNotes = "")
colnames(df_addNew1) <- colnames_dictionary
#============================================================
# New salary fields
#============================================================
# New salary variable field names
xx_fieldNames_salaries <- c("n_salary_scales",
                            "n_flat_rated_salaries",
                            "salary_scales",
                            "flat_salaries",
                            "salary_min",
                            "salary_max",
                            "salScale_loBound",
                            "salScale_upBound",
                            "salScaleDiff")

xx_fieldNames_salaries_GEN <- paste0(xx_fieldNames_salaries, "_GEN")
xx_fieldNames_salaries_DWP <- paste0(xx_fieldNames_salaries, "_DWP")
xx_fieldNames_salaries_DWPandGEN <- c("salary_min_DWPandGEN",
                                      "salary_max_DWPandGEN",
                                      "salary_meanLo_DWPandGEN",
                                      "salary_cvLo_DWPandGEN",
                                      "salScaleDiff_min_DWPandGEN",
                                      "salScaleDiff_max_DWPandGEN",
                                      "salScaleDiff_mean_DWPandGEN",
                                      "salScaleDiff_cv_DWPandGEN")
fieldNames_salaries <- c(xx_fieldNames_salaries_DWP,
                         xx_fieldNames_salaries_GEN,
                         xx_fieldNames_salaries_DWPandGEN)
# New salary variable annotation letters
xx_annotationLetters_salaries <- c("Ns", "Nf", "Ss", "Sf", "S_min", "S_max", "Ss_lo", "Ss_up", "Ss_d")
xx_annotationLetters_salaries_GEN <- paste0(xx_annotationLetters_salaries, "_GEN")
xx_annotationLetters_salaries_DWP <- paste0(xx_annotationLetters_salaries, "_DWP")
xx_annotationLetters_salaries_DWPandGEN <- c("S_min_DWPandGEN",
                                             "S_max_DWPandGEN",
                                             "S_muLo_DWPandGEN",
                                             "S_cvLo_DWPandGEN",
                                             "Ssd_min_DWPandGEN",
                                             "Ssd_max_DWPandGEN",
                                             "Ssd_mu_DWPandGEN",
                                             "Ssd_cv_DWPandGEN")
annotationLetters_salaries <- c(xx_annotationLetters_salaries_DWP,
                                xx_annotationLetters_salaries_GEN,
                                xx_annotationLetters_salaries_DWPandGEN)
# New salary variable descriptions
xx_descriptions_salaries <- c("Number of salary scales offered for this position",
                              "Number of flat-rated salaries offered for this position",
                              "Salary scales offered for this position",
                              "Flat-rated salaries offered for this position",
                              "Lowest salary offered for this position (minimum of Ss and Sf)",
                              "Highest salary offered for this position (maximum of Ss and Sf)",
                              "Salary scale lower bounds for this position",
                              "Salary scale upper bounds for this position",
                              "Difference between salary scale upper and lower bounds (Ss_up - Ss_lo)")
xx_descriptions_salaries_GEN <- paste0(xx_descriptions_salaries, ", GEN only")
xx_descriptions_salaries_DWP <- paste0(xx_descriptions_salaries, ", DWP only")
xx_descriptions_salaries_DWPandGEN <- c("Lowest salary offered for this position (minimum of Ss and Sf)",
                                        "Highest salary offered for this position (maximum of Ss and Sf)",
                                        "Mean salary for this position, calculated over flat-rates (if any) AND the lower bounds of any salary scales (mean of Ss and Sf)",
                                        "Coefficient of variation of all salaries offered for this position, calculated over flat-rates (if any) AND the lower bounds of any salary scales",
                                        "Smallest difference between salary scale upper and lower bounds (minimum of Ss_d)",
                                        "Largest difference between salary scale upper and lower bounds (maximum of Ss_d)",
                                        "Mean difference between salary scale upper and lower bounds (mean of Ss_d)",
                                        "Coefficient of variation of the differences between salary scale upper and lower bounds (coefficient of variation of Ss_d)")
xx_descriptions_salaries_DWPandGEN <- paste0(xx_descriptions_salaries_DWPandGEN, ", DWP and GEN")
descriptions_salaries <- c(xx_descriptions_salaries_DWP,
                           xx_descriptions_salaries_GEN,
                           xx_descriptions_salaries_DWPandGEN)

# New salary variable data type
dataType_salaries <- c(rep(c("Integer", "Integer", "String", "String", rep("Integer", 5)), 2),
                       rep("Integer", 2), rep("Float", 2), rep("Integer", 2), rep("Float", 2))
# New salary variable allowable values
allowValues_salaries <- c(rep(c("Positive integers", "Positive integers", "List of numbers", "List of numbers", rep("Positive integers", 5)), 2),
                          rep("Positive integers", 2), rep("Positive real numbers", 2), rep("Positive integers", 2),
                          rep("Positive real numbers", 2))
# New salary variable accepts null?
acceptsNull_salaries <- rep("Yes", length(fieldNames_salaries))
df_addNew2 <- data.frame(fieldNames_salaries,
                         annotationLetters_salaries,
                         descriptions_salaries,
                         dataType_salaries,
                         allowValues_salaries,
                         acceptsNull_salaries,
                         AddNotes = "")
colnames(df_addNew2) <- colnames_dictionary
#--------------------------------------------------
df_addNew <- rbind(df_addNew1, df_addNew2)
df_dataDictionary <- read.csv(dataDictionary_file_path)
colnames(df_dataDictionary) <- colnames_dictionary
df_dataDictionary <- rbind(df_dataDictionary, df_addNew)
output_file_name <- "Data Dictionary Extended.csv"
output_file_path <- paste0(output_folder, output_file_name)
write.csv(df_dataDictionary, output_file_path)

```

# 3) Semantic alignment analysis of LA City job bulletins

Now that the job class information has been extracted from the bulletins, it's time to put it to use.

## 3.1 Assessing feminine/masculine semantic alignment

Below I assess the feminine/masculine semantic alignment of the bulletin texts. To reduce noise, I focus only on the text unique to each bulletin, generally contained in the Duties, Requirements, Process Notes, and Selection Process sections.

```{r} 
df <- df_outputFile
df$Str <- paste(df$Duties, df$Requirements)
df$Str <- paste(df$Str, df$Process_Notes)
df$Str <- paste(df$Str, df$Selection_Process)

```

As mentioned in the Introduction, the assessment is conducted by converting the Corpus of Contemporary American English (COCA, 2018) into word vectors using Google's word2vec function. A good word vector tutorial can be found here:

https://skymind.ai/wiki/word2vec

Feminine and masculine semantic poles are defined by taking the average of a number of judiciously chosen male and female gendered words. Each bulletin's words are averaged, and then the cosine similarity between the bulletin's average word and the feminine and semantic poles is calculated. The bulletin's feminine semantic alignment is defined as its masculine cosine similarity subtracted from its feminine cosine similarity. The bulletin's masculine semantic alignment is defined as the negative of the feminine semantic alignment.

The COCA used here is a 1.7 million word free sample of the full 560+ million word corpus, which is available for a price (see link in the references for details). Like the full corpus, the free sample used here contains news, magazine, fiction, academic, and spoken transcripts from 1990 to 2017, representing an ample cross section of contemporary American English usage, including, presumably, linguistic embeddings of conceptual gender stereotypes. 

In the following code, I define functions necessary to conduct this analysis: 1) a function to process the COCA and bulletin texts (stem words, remove numbers, URLS, etc.), 2) a function to take the average of several vectors, 3) a function to find the alignment of a given text with respect given semantic poles.

```{r}
# Define functions needed in semantic alignment analysis

# Functions for processing corpora
removeURL <- function(x) gsub("http:[[:alnum:]]*", "", x)
processCorpus <- function(corpus, removePunct = T){
  corpus <- tm_map(corpus, content_transformer(removeURL))
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeNumbers)
  if(removePunct){
  corpus <- tm_map(corpus, removePunctuation, preserve_intra_word_dashes = TRUE)
  }
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  corpus <- tm_map(corpus, stemDocument, language = "english")
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, str_trim)
  return (corpus)
}

# Function to get average word vector
get_average_wordVector <- function(this_semantic_pole, df_wordVecs){
  ind_these_wordVecs <- which(df_wordVecs$Word %in% this_semantic_pole)
  df_these_wordVecs <- df_wordVecs[ind_these_wordVecs, -1]
  average_semantic_vec <- colSums(df_these_wordVecs) / (nrow(df_these_wordVecs) - 1)
}

# Function to capture position of a word vector with respect to given semantic poles (eg. feminine - masculine)
semantic_alignment <- function(txt_to_assess, df_wordVecs, list_semantic_poles, single_word = F, quietly = T){
  
  general_vocab <- df_wordVecs$Word
  if(!single_word){txt_vocab <- unlist(str_split(txt_to_assess, " "))}else{txt_vocab <- txt_to_assess}
  ind_txt_vocab <- which(general_vocab %in% txt_vocab)
  df_txt_vecs <- df_wordVecs[ind_txt_vocab, -1]
  if(!single_word){txtVec <- colSums(df_txt_vecs) / (nrow(df_txt_vecs) - 1)}else{txtVec <- as.numeric(df_txt_vecs)}
  
  pole_1_vec <- list_semantic_poles[[1]]
  pole_2_vec <- list_semantic_poles[[2]]
  
  semantic_alignment_pole_1 <- as.numeric(lsa::cosine(txtVec, pole_1_vec))
  semantic_alignment_pole_2 <- as.numeric(lsa::cosine(txtVec, pole_2_vec))
  
  semantic_score <- semantic_alignment_pole_1 - semantic_alignment_pole_2
  if(quietly == F){
      print(paste("semantic score: ", semantic_score))
  }
  return(semantic_score)
}


```


Next, I prepare bulletins and COCA for analysis (stem words, remove spaces, numbers, punctuation, URLs, stop words, etc.).

```{r}
options(warn = -1, message = FALSE)

bulletin_corpus <- Corpus(VectorSource(df$Str))
bulletin_corpus <- processCorpus(bulletin_corpus)
COCA_file_path <- paste0(COCA_folder, "COCAsample_single_file.txt")
df_COCA <- read.table(COCA_file_path)
COCA_corpus <- Corpus(VectorSource(df_COCA$x))
COCA_corpus <- processCorpus(COCA_corpus)
#COCA_corpus$content[[1]]
#length(COCA_corpus)
```

Next, I convert the COCA to word vectors using Google's word2vec (which in R is rword2vec). I am not aware of any strict guidelines on word vector length. I set word vector length to 200, which seems in line with standard practice.

```{r}
COCA_file_path <- paste0(output_folder, "COCA_corp.txt")
write.table(COCA_corpus$content, COCA_file_path, sep = "\t")
wordvec_length <- 200
#set.seed(1234)
model <- word2vec(layer1_size = wordvec_length, train_file = COCA_file_path, output_file = "vec.bin", binary = 1, num_threads = 128)

```

Once the word vectors have been fitted, they must be placed into a usable format.

```{r}

options(warn = -1, message = FALSE)
bin_to_txt("vec.bin", "vector.txt")
df_wordVecs <- as.data.frame(read_delim("vector.txt", 
                                        skip = 1, delim = " ",
                                        col_names = c("Word", paste0("E", 1:wordvec_length))))
df_wordVecs <- df_wordVecs[-nrow(df_wordVecs), ]

```

Now I define the feminine and masculine semantic poles. The (stemmed) words I choose for the feminine pole are "feminin", "femal", "woman", "women", "communal", and "inclus". For the masculine pole I choose "man", "men", "male", "domin", and "exclus". The poles are defined as the respective averages of these word vectors.

There is lots of room for experimentation in the definition of the semantic poles! Given more time, it would be interesting to see if there is a way to back out "optimal poles" based on some sort of variance minimizing criteria.

```{r}

feminine_semantic_pole <- c("feminin", "femal", "woman", "women", "communal", "inclus")
masculine_semantic_pole <- c("man", "men", "male", "domin", "exclus")
fem_semantic_vec <- get_average_wordVector(feminine_semantic_pole, df_wordVecs)
masc_semantic_vec <- get_average_wordVector(masculine_semantic_pole, df_wordVecs)

```

In the next snippet, I use my semantic_alignment() function to calculate the feminine semantic alignment score of all the job bulletins.

```{r}
list_semantic_poles <- list(fem_semantic_vec, masc_semantic_vec)
outlist <- map(bulletin_corpus$content, semantic_alignment, df_wordVecs, list_semantic_poles)
fem_semanticAlignment_score <- do.call(c, outlist)

```

Let's look at a histogram of the bulletin feminine semantic alignment scores.

```{r}
length(fem_semanticAlignment_score)
nrow(df)
df$fem_semanticAlign_score <- fem_semanticAlignment_score
gg <- ggplot(df, aes(x = fem_semanticAlign_score))
gg <- gg + geom_histogram(color = "magenta", fill = "cyan")
gg <- gg + xlab("Feminine semantic alignment score")
gg

```
The bulk of the bulletins are concentrated at the lower end of the feminine alignment score, with a thin tail stretching into feminine alignment territory.

What are the job classes with the highest feminine semantic alignment? Below I extract those with scores above the 95th percentile.

```{r}

q <- quantile(fem_semanticAlignment_score, probs = 0.95)
ind_top <- which(fem_semanticAlignment_score > as.numeric(q))
ind_top_ordered <- ind_top[order(fem_semanticAlignment_score[ind_top], decreasing = T)]
df_look <- data.frame(Jobs_with_highest_feminine_alignment = df$JOB_CLASS_TITLE[ind_top_ordered],
                      Feminine_Alignment_Score = fem_semanticAlignment_score[ind_top_ordered])
kable(df_look)

```

We see that mostly assistant or otherwise junior level job classes have the highest feminine semantic alignment. Note that many of the jobs are nurse or clerical roles.

What job classes have the highest masculine semantic alignment? Below I extract those with masculine alignment scores above the 95th percentile.

```{r}
q <- quantile(-fem_semanticAlignment_score, probs = 0.95)
ind_bottom <- which(-fem_semanticAlignment_score > as.numeric(q))
ind_bottom_ordered <- ind_bottom[order(fem_semanticAlignment_score[ind_bottom], decreasing = F)]
df_look <- data.frame(Jobs_with_highest_masculine_alignment = df$JOB_CLASS_TITLE[ind_bottom_ordered],
                      Masculine_Alignment_Score = -fem_semanticAlignment_score[ind_bottom_ordered])
kable(df_look)

```

We see that the jobs with the highest masculine semantic alignment score tend to be senior level roles with words like "senior", "director", "chief", "supervisor" in their title. Note also the many "agentic" roles like engineer and pilot.

How do salaries differ between the feminine and masculine aligned job classes?

```{r}
mean(df$salary_min_GEN[ind_top], na.rm = T)
mean(df$salary_min_GEN[ind_bottom], na.rm = T)
mean(df$salary_min_DWP[ind_top], na.rm = T)
mean(df$salary_min_DWP[ind_bottom], na.rm = T)

kable(paste("Mean salary (GEN) of jobs with highest feminine semantic alignment:", mean(df$salary_min_GEN[ind_top], na.rm = T)))
kable(paste("Mean salary (GEN) of jobs with highest masculine semantic alignment:", mean(df$salary_min_GEN[ind_bottom], na.rm = T)))

kable(paste("Mean salary (DWP) of jobs with highest feminine semantic alignment:", mean(df$salary_min_DWP[ind_top], na.rm = T)))
kable(paste("Mean salary (DWP) of jobs with highest masculine semantic alignment:", mean(df$salary_min_DWP[ind_bottom], na.rm = T)))

```
There is quite a breach in GEN salaries between the two groups. The job classes with high feminine semantic alignment pay substantially less than jobs with high masculine semantic alignment. There is also disparity between DWP salaries for the two groups, although less pronounced.

This motivates a closer look. Is there a statistically significant relation between the feminine/masculine semantic alignment of bulletin texts and salary levels? To investigate, below I estimate the following model:

$$
\mathrm{Feminine}\:\:\mathrm{Alignment}\:\:\mathrm{Score} = \beta_0 + \beta_{\log(\mathrm{Salary})}\log(\mathrm{Salary}) + \beta_{\log(Nwords)}\log(N_{words}) + \beta_{\log(Nsent)}\log(N_{sent}) + \epsilon
$$

where $N_{words}$ and $N_{sent}$ are the word and sentence counts, respectively, of each bulletin's unique text (the Duties, Requirements, Process Notes, and Selection Process sections). The salary variable in this model corresponds to the maximum salary (flat-rated or scale, DWP or GEN) indicated in each bulletin.

```{r}
# Take logs for regression model
df$log_salary_min_DWPandGEN <- log(df$salary_min_DWPandGEN)
u <- df$log_salary_min_DWPandGEN
df$FILE_NAME[which(is.nan(u))]
df$FILE_NAME[which(is.infinite(u))]
df$log_salary_max_DWPandGEN <- log(df$salary_max_DWPandGEN)
u <- df$log_salary_max_DWPandGEN
df$FILE_NAME[which(is.nan(u))]
df$FILE_NAME[which(is.infinite(u))]
df$log_salary_meanLo_DWPandGEN <- log(df$salary_meanLo_DWPandGEN)
u <- df$log_salary_meanLo_DWPandGEN
df$FILE_NAME[which(is.nan(u))]
df$FILE_NAME[which(is.infinite(u))]
df$log_salScaleDiff_min_DWPandGEN <- log(df$salScaleDiff_min_DWPandGEN)
u <- df$log_salScaleDiff_min_DWPandGEN
df$FILE_NAME[which(is.nan(u))]
df$FILE_NAME[which(is.infinite(u))]
df$log_salScaleDiff_max_DWPandGEN <- log(df$salScaleDiff_max_DWPandGEN)
u <- df$log_salScaleDiff_max_DWPandGEN
df$FILE_NAME[which(is.nan(u))]
df$FILE_NAME[which(is.infinite(u))]
df$log_num_words <- log(df$num_words_total)
df$log_num_sents <- log(df$num_sents_total)
df$log_words_per_sent <- log(df$num_words_total / df$num_sents_total)
#------------------------------------------------------
mod <- lm(fem_semanticAlign_score ~ log_salary_max_DWPandGEN +
            log_num_words + log_num_sents, df)
summary(mod)
m_table <- tidy(mod)
kable(m_table, "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "responsive"))

```
The coefficients are all statistically significant $(p < 0.001)$.

The coefficients on the logged variables can be interpreted as percentages (if multiplied by 100). The estimation results thus suggest that salaries decrease by roughly 1% for every marginal increase in the feminine semantic alignment.

There is inherent randomness in the rword2vec() function (which cannot be managed through set.seed()), and so these results vary a little from one run to another. Nonetheless, after several runs, I find that the -1% result holds to within about +/-0.7%.

The regression also reveals a significant relation between feminine semantic alignment and word and sentence length. Specifically, feminine semantic alignment is negatively correlated with word count, but also positively correlated with sentence count.

I noticed that many bulletins have extremely long sentences enumerating lists of duties or required skills separated by semicolons. The coefficients on the word and sentence count variables in this regression analysis suggest that such sentences may deter female applicants. To attract female applicants, bulletins should not merely be shorter but also more concise, i.e. expressing the same information in fewer words (purged of jargon, and so forth), broken up over more sentences.

## 3.2 Assessing the semantic alignment of a single bulletin in detail

The feminine semantic alignment of a single bulletin can be assessed sentence by sentence as follows.

```{r}
#------------------------------------------------------------------
# Select a bulletin to assess in detail
this_bulletin <- df$Str[234]
print(df$JOB_CLASS_TITLE[234])
#------------------------------------------------------------------
sents_to_assess_char <- unlist(tokenize_sentences(this_bulletin))
sents_to_assess <- Corpus(VectorSource(sents_to_assess_char))
sents_to_assess <- processCorpus(sents_to_assess, removePunct = T)
ind_rm <- which(sents_to_assess$content == "")
if(length(ind_rm) != 0){sents_to_assess$content <- sents_to_assess$content[-ind_rm]}
outlist <- map(sents_to_assess$content, semantic_alignment, df_wordVecs, list_semantic_poles)
fem_semanticAlignment_score <- do.call(c, outlist)
df_look <- data.frame(Sentence_to_assess = sents_to_assess_char[-ind_rm], Feminine_Semantic_Alignment_Score = fem_semanticAlignment_score)
kabble(df_look[1:15, ])

```
Or, the semantic alignment of a single bulletin can be assessed word by word as follows.

```{r}
words_to_assess <- Corpus(VectorSource(this_bulletin))
words_to_assess <- processCorpus(words_to_assess, removePunct = T)
words_to_assess <- unlist(str_split(words_to_assess, " "))
outlist <- map(words_to_assess, semantic_alignment, df_wordVecs, list_semantic_poles, single_word = T)
fem_semanticAlignment_score <- do.call(c, outlist)
ind_rm <- which(is.na(fem_semanticAlignment_score))
df_look <- data.frame(Word_to_assess = words_to_assess[-ind_rm], Feminine_Semantic_Alignment_Score = fem_semanticAlignment_score[-ind_rm])
kabble(df_look[1:15, ])

```

With some front end development, this could be turned into an app with an interface comparable to that of Textio.


## 3.2 Assessing racial semantic alignment

It is not clear how the list-based approach to semantic gender alignment assessment might be scaled out to other forms of bias, particularly racial bias. It is acceptable to ellicit, compile, and curate lists of feminine and masculine traits, but any attempt to do the same for racial traits will run into controversey. Historically, attempts to do so have been associated with pathological social and academic movements.

The corpus based method proposed above may provide a way forward. If it is true, as the gender bias literature theorizes, that language embeds and perpetuates gender stereotypes, then it is also probably true that language does the same for racial stereotypes. Instead of elliciting lists of racial traits from surveys, then, why not ellicit them from the corpus?

The rword2vec package provides functions to retrieve semantically "nearby" or analagous words given a user-defined word or group of words. We can thus begin with a crudely defined pole consisting of just a couple words, and then build on it by searching the word vector space for semantic neighbors.

For example, a rudimentary definition for a Latino semantic pole might be something like: "latino", "hispanic", "non-whit", "american". Next, we search for semantic neighbors as follows.

```{r}
latino_semantic_pole <- c("hispan", "latin", "non-whit", "american")
df_semantic_neighbors_latino <- word_analogy(file_name = "vec.bin", search_word = paste(latino_semantic_pole, collapse = " "), num = 20)
kable(df_semantic_neighbors_latino)

```
This list of semantic neighbors contains some words whose relation to Latino culture may not be clear. But that's ok. We are looking at a linguistic fossil of a cultural stereotype. Fossils often preserve things in messy ways. Also, these unexpected words may reflect unconscious associations.

I augment the Latino semantic pole with the top ten words on the semantic neighbors list.

```{r}
latino_semantic_pole <- c(latino_semantic_pole, as.character(df_semantic_neighbors_latino$word[1:10]))

```
Now, the polar opposite of Latino must also be defined. This is less straightforward than defining the semantic opposite of "feminine". The corpus based method requires us to think carefully about what we really mean by "diversity" and "bias". For now I will define the opposite of Latino as all other non-Latino racial groups with a substantial presence in LA. But this should be subjected to careful thought and experimentation.

```{r}

nonLatino_semantic_pole <- c("white", "asian", "african-american", "american")
df_semantic_neighbors_nonLatino <- word_analogy(file_name = "vec.bin", search_word = paste(nonLatino_semantic_pole, collapse = " "), num = 10)
kable(df_semantic_neighbors_nonLatino)
nonLatino_semantic_pole <- c(nonLatino_semantic_pole, as.character(df_semantic_neighbors_nonLatino$word[1:10]))

```
Next, I numerically define the Latino and non_Latino semantic poles by averaging their respective word vectors.

```{r}

latino_semantic_vec <- get_average_wordVector(latino_semantic_pole, df_wordVecs)
nonLatino_semantic_vec <- get_average_wordVector(nonLatino_semantic_pole, df_wordVecs)

```

I once again use my semantic_alignment() function, this time to calculate the Latino semantic alignment score of all the job bulletins.

```{r}

list_semantic_poles <- list(latino_semantic_vec, nonLatino_semantic_vec)
outlist <- map(bulletin_corpus$content, semantic_alignment, df_wordVecs, list_semantic_poles)
latino_semanticAlignment_score <- do.call(c, outlist)

```
Now I can conduct the same analysis as I did above for feminine semantic alignment, but this time it is for Latino semantic alignment.

Let's look at a histogram of the bulletin Latino semantic alignment scores.

```{r}

df$latino_semanticAlign_score <- latino_semanticAlignment_score
gg <- ggplot(df, aes(x = latino_semanticAlign_score))
gg <- gg + geom_histogram(color = "magenta", fill = "cyan")
gg <- gg + xlab("Latino semantic alignment score")
gg

```
The bulk of the bulletins are concentrated at the upper end of the Latino alignment score, with a thin tail stretching into non-Latino alignment.

What are the job classes with the highest Latino semantic alignment? Below I extract those with scores above the 95th percentile.

```{r}
this_semanticAlignment_score <- latino_semanticAlignment_score
q <- quantile(this_semanticAlignment_score, probs = 0.95)
ind_top <- which(this_semanticAlignment_score > as.numeric(q))
ind_top_ordered <- ind_top[order(this_semanticAlignment_score[ind_top], decreasing = T)]
df_look <- data.frame(Jobs_with_highest_Latino_alignment = df$JOB_CLASS_TITLE[ind_top_ordered],
                      Latino_Alignment_Score = this_semanticAlignment_score[ind_top_ordered])
kable(df_look)

```

The job classes with the highest Latino semantic alignment scores vary, but a few patterns can be discerned. Some of the jobs are public outreach and communication roles, which makes sense given that Latinos constitute the largest cultural segment in LA. There are some assistant level jobs, but these are outweighed by senior level police or bureaucratic roles.

What job classes have the highest non-Latino semantic alignment? Below I extract those with non-Latino alignment scores above the 95th percentile.

```{r}

q <- quantile(-this_semanticAlignment_score, probs = 0.95)
ind_bottom <- which(-this_semanticAlignment_score > as.numeric(q))
ind_bottom_ordered <- ind_bottom[order(this_semanticAlignment_score[ind_bottom], decreasing = F)]
df_look <- data.frame(Jobs_with_highest_nonLatino_alignment = df$JOB_CLASS_TITLE[ind_bottom_ordered],
                      nonLatino_Alignment_Score = -this_semanticAlignment_score[ind_bottom_ordered])
kable(df_look)

```

The job classes with lowest Latino semantic alignment are mostly journeyman jobs. Why might this be?

How do salaries differ between the latino and caucasian aligned job classes?

```{r}
mean(df$salary_min_GEN[ind_top], na.rm = T)
mean(df$salary_min_GEN[ind_bottom], na.rm = T)
mean(df$salary_min_DWP[ind_top], na.rm = T)
mean(df$salary_min_DWP[ind_bottom], na.rm = T)

kable(paste("Mean salary (GEN) of jobs with highest Latino semantic alignment:", mean(df$salary_min_GEN[ind_top], na.rm = T)))
kable(paste("Mean salary (GEN) of jobs with lowest Latino semantic alignment:", mean(df$salary_min_GEN[ind_bottom], na.rm = T)))

kable(paste("Mean salary (DWP) of jobs with highest Latino semantic alignment:", mean(df$salary_min_DWP[ind_top], na.rm = T)))
kable(paste("Mean salary (DWP) of jobs with lowest Latino semantic alignment:", mean(df$salary_min_DWP[ind_bottom], na.rm = T)))

```

There is a considerable salary disparity between the two groups, especially as regards DWP salaries. The job classes with high Latino semantic alignment pay more than jobs with low Latino alignment.

Again, let's take a closer look via regression analysis.



```{r}
mod <- lm(latino_semanticAlign_score ~ log_salary_max_DWPandGEN +
            log_num_words + log_num_sents, df)
summary(mod)
m_table <- tidy(mod)
kable(m_table, "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "responsive"))

```

Again, all estimated coefficients are statistically significant.

The coefficients suggest that higher Latino semantic alignment is correlated with higher salaries. The coefficients on the logged word and sentence counts suggest that bulletins with high word counts and long sentences are less semantically aligned with Latinos.

So, why might all this be? Are Latinos more upwardly mobile compared to non-Latinos? Is their emergence into higher payscales a natural result of their cultural predominance in LA?

Unlike gender semantic bias detection, racial semantic bias detection is a new area. The corpus based semantic alignment method proposed above can help us to ask the right questions as we begin to explore it.


```{r}
rm(model, COCA_file_path, "vec.bin", "vector.txt")
```

# References

Askehave, I., & Zethsen, K. K. (2014). Gendered Constructions of Leadership in D anish Job Advertisements. Gender, Work & Organization, 21(6), 531-545.

Bem, S. L., & Bem, D. J. (1973). Does Sex‐biased Job Advertising “Aid and Abet” Sex Discrimination? 1. Journal of Applied Social Psychology, 3(1), 6-18.

Corpus of Contemporary American English (COCA). (2018). Free sample. [115 data files]. Retrieved from https://www.corpusdata.org/formats.asp.

Donnelly, K., & Twenge, J. M. (2017). Masculine and feminine traits on the Bem Sex-Role Inventory, 1993–2012: A cross-temporal meta-analysis. Sex Roles, 76(9-10), 556-565.

Gaucher, D., Friesen, J., & Kay, A. C. (2011). Evidence that gendered wording in job advertisements exists and sustains gender inequality. Journal of personality and social psychology, 101(1), 109.

Hannak, A., Wagner, C., Garcia, D., Mislove, A., Strohmaier, M., & Wilson, C. (2017, February). Bias in online freelance marketplaces: Evidence from taskrabbit and fiverr. In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing (pp. 1914-1933). ACM.

Tang, S., Zhang, X., Cryan, J., Metzger, M. J., Zheng, H., & Zhao, B. Y. (2017). Gender bias in the job market: A longitudinal analysis. Proceedings of the ACM on Human-Computer Interaction, 1(CSCW), 99.

```
